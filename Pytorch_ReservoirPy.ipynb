{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install reservoirpy torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFH5T8fYczxW",
        "outputId": "ea2bea35-7fe1-4d6e-cca2-f7bb1aa2ffa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reservoirpy\n",
            "  Downloading reservoirpy-0.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (1.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading reservoirpy-0.4.1-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reservoirpy\n",
            "Successfully installed reservoirpy-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from reservoirpy.nodes import Reservoir\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiTltUkWc4PH",
        "outputId": "dc421a98-d23a-4e6e-b6f4-6abcf3ddddca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ReservoirPy states\n",
        "def compute_reservoir_states(reservoir, X_images):\n",
        "    \"\"\"\n",
        "    X_images: numpy array shape [N_samples, 28, 28]\n",
        "    Produce for each sample a flattened vector of states: [N_units * T]\n",
        "    Return: states numpy array shape [N_samples, N_units * T]\n",
        "    \"\"\"\n",
        "    n_samples = X_images.shape[0]\n",
        "    T, in_dim = X_images.shape[1], X_images.shape[2]\n",
        "    # collect states per sample\n",
        "    states_out = []\n",
        "    for i in range(n_samples):\n",
        "        seq = X_images[i]  # shape [T, in_dim]\n",
        "        # reservoir.run expects shape (T, in_dim)\n",
        "        out = reservoir.run(seq)  # returns shape [T, n_units] (states per timestep)\n",
        "        # flatten timesteps and units into single vector\n",
        "        flat = out.reshape(-1)  # size n_units * T\n",
        "        states_out.append(flat)\n",
        "    return np.vstack(states_out).astype(np.float32)  # shape [n_samples, n_units*T]\n"
      ],
      "metadata": {
        "id": "8RuoQg5wdDLi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST format as sequences\n",
        "def load_mnist_seq(train=True):\n",
        "    # Download raw MNIST images as [N, 28, 28] floats in [0,1]\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_set  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    X_tr = train_set.data.numpy().astype(np.float32) / 255.0  # [60000,28,28]\n",
        "    Y_tr = np.eye(10)[train_set.targets.numpy()]             # one-hot [60000,10]\n",
        "    X_te = test_set.data.numpy().astype(np.float32) / 255.0   # [10000,28,28]\n",
        "    Y_te = np.eye(10)[test_set.targets.numpy()]\n",
        "    return X_tr, Y_tr.astype(np.float32), X_te, Y_te.astype(np.float32)"
      ],
      "metadata": {
        "id": "ZkKveSIkdWjl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RluNvtgOcarH"
      },
      "outputs": [],
      "source": [
        "# SpaRCe model\n",
        "class SpaRCeModel(nn.Module):\n",
        "    def __init__(self, in_features, n_classes, theta_i_init=None, theta_g=None):\n",
        "        \"\"\"\n",
        "        in_features: size [N_units * T] - flattened reservoir states\n",
        "        n_classes: number of output classes\n",
        "        theta_i_init: numpy array shape (in_features,) initial theta_i\n",
        "        theta_g: numpy array shape (in_features,) fixed global threshold\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # linear readout\n",
        "        self.readout = nn.Linear(in_features, n_classes, bias=False)\n",
        "\n",
        "        # local threshold theta_i trainable per neuron\n",
        "        if theta_i_init is None:\n",
        "            theta_i_init = np.random.randn(in_features).astype(np.float32) / (in_features**0.5)\n",
        "        self.theta_i = nn.Parameter(torch.from_numpy(theta_i_init).float())  # shape [in_features]\n",
        "\n",
        "        # fixed global threshold theta_g (non-trainable)\n",
        "        if theta_g is None:\n",
        "            theta_g = np.zeros((in_features,), dtype=np.float32)\n",
        "        self.register_buffer(\"theta_g\", torch.from_numpy(theta_g).float())  # buffer, not parameter\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        state: tensor [batch, in_features] (raw reservoir states)\n",
        "        returns logits [batch, n_classes] and state_sparse [batch, in_features]\n",
        "        Activation: state_sparse = sign(state) * relu(abs(state) - theta_g - theta_i)\n",
        "        \"\"\"\n",
        "        # broadcast thresholds\n",
        "        th = (self.theta_g + self.theta_i).unsqueeze(0)  # [1, in_features]\n",
        "        s_abs = torch.abs(state)\n",
        "        s_thresh = F.relu(s_abs - th)  # [batch, in_features]\n",
        "        state_sparse = torch.sign(state) * s_thresh\n",
        "        logits = self.readout(state_sparse)\n",
        "        return logits, state_sparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of ESN or SpaRCe\n",
        "def train_models(States_tr, Y_tr, States_te, Y_te, States_val=None,\n",
        "                 MODEL=1,            # 1 = SpaRCe, 2 = Standard ESN\n",
        "                 Pns_list=[70],      # percentiles (only for SpaRCe)\n",
        "                 alpha_list=[1e-3],  # learning rates (for Standard ESN can be list)\n",
        "                 batch_size=128,\n",
        "                 N_episodes=2000,\n",
        "                 N_check=20,\n",
        "                 device=device):\n",
        "    \"\"\"\n",
        "    States_*: numpy arrays of shape [N_samples, in_features]\n",
        "    Y_*: numpy arrays (one-hot vectors) of shape [N_samples, n_classes]\n",
        "    Returns: Results_tr, Results_te, Results_val arrays\n",
        "    \"\"\"\n",
        "    States_tr = States_tr.astype(np.float32)\n",
        "    States_te = States_te.astype(np.float32)\n",
        "    if States_val is not None:\n",
        "        States_val = States_val.astype(np.float32)\n",
        "\n",
        "    N_train = States_tr.shape[0]\n",
        "    N_test  = States_te.shape[0]\n",
        "    N_class = Y_tr.shape[1]\n",
        "    in_features = States_tr.shape[1]\n",
        "\n",
        "    # splits for evaluation: mirroring original code logic\n",
        "    train_divide = 100\n",
        "    test_divide  = 50\n",
        "    val_divide   = 50\n",
        "\n",
        "    N_train_d = int(np.floor(N_train / train_divide))\n",
        "    N_test_d  = int(np.floor(N_test / test_divide))\n",
        "    if States_val is not None:\n",
        "        N_val = States_val.shape[0]\n",
        "        N_val_d = int(np.floor(N_val / val_divide))\n",
        "    else:\n",
        "        N_val = 0\n",
        "        N_val_d = 0\n",
        "\n",
        "    # prepare PyTorch datasets for training (will sample random indices)\n",
        "    train_dataset = TensorDataset(torch.from_numpy(States_tr), torch.from_numpy(Y_tr))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    # prepare results containers\n",
        "    if MODEL == 1:\n",
        "        N_copies = len(Pns_list)\n",
        "        Results_tr = np.zeros((N_copies, N_check, 3), dtype=np.float32)  # [error, accuracy, coding]\n",
        "        Results_te = np.zeros((N_copies, N_check, 3), dtype=np.float32)\n",
        "        Results_val= np.zeros((N_copies, N_check, 3), dtype=np.float32)\n",
        "    else:\n",
        "        N_copies = len(alpha_list)\n",
        "        Results_tr = np.zeros((N_copies, N_check, 2), dtype=np.float32)  # [error, accuracy]\n",
        "        Results_te = np.zeros((N_copies, N_check, 2), dtype=np.float32)\n",
        "        Results_val= np.zeros((N_copies, N_check, 2), dtype=np.float32)\n",
        "\n",
        "    # loss function: sigmoid cross entropy (BCEWithLogits for multi-label)\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    # function to evaluate a model on a dataset by splitting into smaller parts called 'divisions'\n",
        "    def evaluate_model(model, States, Y, division):\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_count = 0\n",
        "        coding_sum = 0.0\n",
        "        n_parts = division\n",
        "        part_size = int(np.floor(States.shape[0] / n_parts)) if n_parts>0 else States.shape[0]\n",
        "        # iterate parts\n",
        "        for part in range(n_parts):\n",
        "            start = part * part_size\n",
        "            end = (part + 1) * part_size\n",
        "            Xp = torch.from_numpy(States[start:end]).to(device)\n",
        "            Yp = torch.from_numpy(Y[start:end]).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits, state_sparse = model(Xp)\n",
        "                loss = criterion(logits, Yp)\n",
        "                total_loss += loss.item() * (end - start)\n",
        "                preds = torch.argmax(torch.sigmoid(logits), dim=1)\n",
        "                labels = torch.argmax(Yp, dim=1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_count += (end - start)\n",
        "                # coding level (fraction of non-zero activations)\n",
        "                if hasattr(model, \"theta_i\"):\n",
        "                    coding_sum += (state_sparse != 0).float().sum().item()\n",
        "        # processing the rest\n",
        "        rem = States.shape[0] - n_parts * part_size\n",
        "        if rem > 0:\n",
        "            start = n_parts * part_size\n",
        "            Xp = torch.from_numpy(States[start:]).to(device)\n",
        "            Yp = torch.from_numpy(Y[start:]).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits, state_sparse = model(Xp)\n",
        "                loss = criterion(logits, Yp)\n",
        "                total_loss += loss.item() * rem\n",
        "                preds = torch.argmax(torch.sigmoid(logits), dim=1)\n",
        "                labels = torch.argmax(Yp, dim=1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_count += rem\n",
        "                if hasattr(model, \"theta_i\"):\n",
        "                    coding_sum += (state_sparse != 0).float().sum().item()\n",
        "        avg_loss = total_loss / total_count\n",
        "        acc = total_correct / total_count\n",
        "        coding = None\n",
        "        if hasattr(model, \"theta_i\"):\n",
        "            coding = coding_sum / (total_count * (in_features / States.shape[1]) )  # simplified: fraction of non-zero elements per sample\n",
        "            # compute coding fraction per-sample\n",
        "            coding = coding_sum / (total_count * in_features)\n",
        "        model.train()\n",
        "        return avg_loss, acc, coding\n",
        "\n",
        "    # MAIN LOOP: train separate models per copy\n",
        "    for copy_idx in range(N_copies):\n",
        "        print(f\"\\n=== Training copy {copy_idx+1}/{N_copies} ===\")\n",
        "        # build model and optimizers depending on MODEL\n",
        "        # if model is SpaRCe\n",
        "        if MODEL == 1:\n",
        "            # compute theta_g_start for this copy (percentile across training states per feature)\n",
        "            P = Pns_list[copy_idx]\n",
        "            # states_tr of shape [N_train, in_features] -> compute percentile per feature\n",
        "            theta_g = np.percentile(np.abs(States_tr), P, axis=0).astype(np.float32)  # shape [in_features,]\n",
        "            theta_i_init = (np.random.randn(in_features).astype(np.float32) / max(1.0, in_features**0.5))\n",
        "            model = SpaRCeModel(in_features, N_class, theta_i_init=theta_i_init, theta_g=theta_g).to(device)\n",
        "            # optimizers: one for readout (output layer), one for theta_i (trainable neural thresholds)\n",
        "            alpha = 1e-3 if len(alpha_list)==0 else alpha_list[0]\n",
        "            opt_readout = torch.optim.Adam(model.readout.parameters(), lr=alpha)\n",
        "            opt_theta   = torch.optim.Adam([model.theta_i], lr=alpha/10.0)\n",
        "            # step both optimizers each batch\n",
        "        else:\n",
        "            # Standard ESN: no trainable theta_i, only linear readout\n",
        "            model = nn.Linear(in_features, N_class, bias=False).to(device)\n",
        "            # wrap into a small wrapper to reuse evaluation code expecting model(X)->(logits,state_sparse)\n",
        "            class StdWrapper(nn.Module):\n",
        "                def __init__(self, linear):\n",
        "                    super().__init__()\n",
        "                    self.linear = linear\n",
        "                def forward(self, state):\n",
        "                    logits = self.linear(state)\n",
        "                    return logits, None\n",
        "            model = StdWrapper(model)\n",
        "            # pick learning rate for this copy\n",
        "            alpha = alpha_list[copy_idx]\n",
        "            opt_readout = torch.optim.Adam(model.parameters(), lr=alpha)\n",
        "            opt_theta = None\n",
        "\n",
        "        # training loop with checkpoints\n",
        "        check_interval = max(1, int(np.round(N_episodes / N_check)))\n",
        "        train_iter = trange(N_episodes, desc=f\"Copy {copy_idx+1}\")\n",
        "        for n in train_iter:\n",
        "            # one mini-batch update (sample random batch)\n",
        "            batch_idx = np.random.randint(0, N_train, size=(batch_size,))\n",
        "            batch_x = torch.from_numpy(States_tr[batch_idx]).to(device)\n",
        "            batch_y = torch.from_numpy(Y_tr[batch_idx]).to(device)\n",
        "            # forward\n",
        "            logits, state_sparse = model(batch_x)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            # backward and update\n",
        "            opt_readout.zero_grad()\n",
        "            if opt_theta is not None:\n",
        "                opt_theta.zero_grad()\n",
        "            loss.backward()\n",
        "            opt_readout.step()\n",
        "            if opt_theta is not None:\n",
        "                opt_theta.step()\n",
        "\n",
        "            # checkpoint evaluation\n",
        "            if n % check_interval == 0:\n",
        "                idx = int(n / check_interval)\n",
        "                # evaluate on test/train/val as authors do\n",
        "                # test\n",
        "                if MODEL == 2:\n",
        "                    loss_te, acc_te, _ = evaluate_model(model, States_te, Y_te, test_divide)\n",
        "                    loss_tr, acc_tr, _ = evaluate_model(model, States_tr, Y_tr, train_divide)\n",
        "                    if States_val is not None:\n",
        "                        loss_val, acc_val, _ = evaluate_model(model, States_val, Y_val, val_divide)\n",
        "                    else:\n",
        "                        loss_val, acc_val = 0.0, 0.0\n",
        "                    Results_te[copy_idx, idx, 0] = loss_te\n",
        "                    Results_te[copy_idx, idx, 1] = acc_te\n",
        "                    Results_tr[copy_idx, idx, 0] = loss_tr\n",
        "                    Results_tr[copy_idx, idx, 1] = acc_tr\n",
        "                    if States_val is not None:\n",
        "                        Results_val[copy_idx, idx, 0] = loss_val\n",
        "                        Results_val[copy_idx, idx, 1] = acc_val\n",
        "                    print(f\"[ESN] Iter {n} copy {copy_idx} TEST acc={acc_te:.4f} loss={loss_te:.4f}\")\n",
        "                else:\n",
        "                    loss_te, acc_te, coding_te = evaluate_model(model, States_te, Y_te, test_divide)\n",
        "                    loss_tr, acc_tr, coding_tr = evaluate_model(model, States_tr, Y_tr, train_divide)\n",
        "                    if States_val is not None:\n",
        "                        loss_val, acc_val, coding_val = evaluate_model(model, States_val, Y_val, val_divide)\n",
        "                    else:\n",
        "                        loss_val, acc_val, coding_val = 0.0, 0.0, 0.0\n",
        "                    Results_te[copy_idx, idx, 0] = loss_te\n",
        "                    Results_te[copy_idx, idx, 1] = acc_te\n",
        "                    Results_te[copy_idx, idx, 2] = coding_te if coding_te is not None else 0.0\n",
        "                    Results_tr[copy_idx, idx, 0] = loss_tr\n",
        "                    Results_tr[copy_idx, idx, 1] = acc_tr\n",
        "                    Results_tr[copy_idx, idx, 2] = coding_tr if coding_tr is not None else 0.0\n",
        "                    if States_val is not None:\n",
        "                        Results_val[copy_idx, idx, 0] = loss_val\n",
        "                        Results_val[copy_idx, idx, 1] = acc_val\n",
        "                        Results_val[copy_idx, idx, 2] = coding_val if coding_val is not None else 0.0\n",
        "                    print(f\"[SpaRCe] Iter {n} copy {copy_idx} TEST acc={acc_te:.4f} loss={loss_te:.4f} coding={coding_te:.4f}\")\n",
        "\n",
        "    return Results_tr, Results_te, Results_val\n"
      ],
      "metadata": {
        "id": "CUPqj8wIdemc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running the whole program\n",
        "if __name__ == \"__main__\":\n",
        "    # Load MNIST and convert to sequences - columns\n",
        "    X_tr, Y_tr, X_te, Y_te = load_mnist_seq()\n",
        "    # reshape to [N, T, in_dim] where T=28 (timesteps), in_dim=28 (features per timestep)\n",
        "    X_tr_seq = X_tr.reshape(-1, 28, 28)\n",
        "    X_te_seq = X_te.reshape(-1, 28, 28)\n",
        "\n",
        "    # build an ESN with reservoirpy\n",
        "    # hyperparams (example)\n",
        "    N_units = 300         # reservoir size\n",
        "    sr = 0.97             # spectral radius\n",
        "    input_scaling = 1.0   # gamma\n",
        "    density = 0.05        # internal sparsity\n",
        "\n",
        "    # create ReservoirPy Reservoir node\n",
        "    res = Reservoir(units=N_units, sr=sr, input_scaling=input_scaling, input_connectivity=density, activation=np.tanh)\n",
        "\n",
        "    # compute reservoir states\n",
        "    print(\"Computing reservoir states for training set...\")\n",
        "    S_tr = compute_reservoir_states(res, X_tr_seq)  # shape [N_train, N_units*T]\n",
        "    print(\"Computing reservoir states for test set...\")\n",
        "    S_te = compute_reservoir_states(res, X_te_seq)\n",
        "\n",
        "    # example: train SpaRCe (MODEL=1) with two Pns (70% and 90%)\n",
        "    Pns_list = [70]           # list of percentiles to test\n",
        "    alpha_list = [1e-3]       # learning rate used for readout (SpaRCe uses alpha and alpha/10 for theta)\n",
        "    Results_tr, Results_te, Results_val = train_models(S_tr, Y_tr, S_te, Y_te,\n",
        "                                                       States_val=None,\n",
        "                                                       MODEL=1,\n",
        "                                                       Pns_list=Pns_list,\n",
        "                                                       alpha_list=alpha_list,\n",
        "                                                       batch_size=128,\n",
        "                                                       N_episodes=1000,\n",
        "                                                       N_check=10,\n",
        "                                                       device=device)\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    print(\"Results (test):\", Results_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm7XBlcTdzWW",
        "outputId": "a07b1ca9-ff03-4741-9578-d5418e55635b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing reservoir states for training set...\n",
            "Computing reservoir states for test set...\n",
            "\n",
            "=== Training copy 1/1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:   1%|▏         | 13/1000 [00:03<03:03,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 0 copy 0 TEST acc=0.2216 loss=0.6071 coding=0.3025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  12%|█▏        | 118/1000 [00:07<01:06, 13.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 100 copy 0 TEST acc=0.8740 loss=0.1144 coding=0.3124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  21%|██        | 209/1000 [00:12<01:17, 10.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 200 copy 0 TEST acc=0.9046 loss=0.0831 coding=0.3205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  32%|███▏      | 322/1000 [00:16<00:52, 12.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 300 copy 0 TEST acc=0.9155 loss=0.0703 coding=0.3262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  41%|████      | 409/1000 [00:20<00:52, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 400 copy 0 TEST acc=0.9240 loss=0.0626 coding=0.3305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  52%|█████▏    | 518/1000 [00:25<00:34, 14.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 500 copy 0 TEST acc=0.9278 loss=0.0578 coding=0.3339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  62%|██████▏   | 615/1000 [00:28<00:22, 17.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 600 copy 0 TEST acc=0.9325 loss=0.0544 coding=0.3366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  71%|███████   | 711/1000 [00:32<00:19, 14.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 700 copy 0 TEST acc=0.9382 loss=0.0512 coding=0.3389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  82%|████████▏ | 817/1000 [00:37<00:15, 11.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 800 copy 0 TEST acc=0.9397 loss=0.0488 coding=0.3409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  91%|█████████ | 911/1000 [00:41<00:05, 15.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 900 copy 0 TEST acc=0.9457 loss=0.0467 coding=0.3425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1: 100%|██████████| 1000/1000 [00:42<00:00, 23.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n",
            "Results (test): [[[0.607086   0.2216     0.30246806]\n",
            "  [0.11438447 0.874      0.3123698 ]\n",
            "  [0.08308282 0.9046     0.32051343]\n",
            "  [0.07025962 0.9155     0.32620943]\n",
            "  [0.06258667 0.924      0.33048436]\n",
            "  [0.05782694 0.9278     0.33387926]\n",
            "  [0.05438729 0.9325     0.33658767]\n",
            "  [0.05115807 0.9382     0.33888468]\n",
            "  [0.04875993 0.9397     0.34090406]\n",
            "  [0.04669606 0.9457     0.34249923]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}